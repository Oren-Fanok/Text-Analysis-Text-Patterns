{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook holds the starting point for your Pattern Assignment. I'll put the function stub at the top, but I have some potentially helpful examples down below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     C:\\Users\\ofano\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('gutenberg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Introductory Examples for the NLTK Book ***\n",
      "Loading text1, ..., text9 and sent1, ..., sent9\n",
      "Type the name of the text or sentence to view it.\n",
      "Type: 'texts()' or 'sents()' to list the materials.\n",
      "text1: Moby Dick by Herman Melville 1851\n",
      "text2: Sense and Sensibility by Jane Austen 1811\n",
      "text3: The Book of Genesis\n",
      "text4: Inaugural Address Corpus\n",
      "text5: Chat Corpus\n",
      "text6: Monty Python and the Holy Grail\n",
      "text7: Wall Street Journal\n",
      "text8: Personals Corpus\n",
      "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "\n",
    "from nltk.book import *\n",
    "from string import punctuation\n",
    "from collections import Counter\n",
    "\n",
    "from pprint import pprint # get some prettier printing of objects\n",
    "\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "\n",
    "sw = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_patterns(text)  :\n",
    "    \"\"\"\n",
    "        This function takes text as an input and returns a dictionary of statistics,\n",
    "        after cleaning the text. \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # We'll make things a big clearer by initializing the \n",
    "    # statistics here. These are placeholder values.\n",
    "    total_tokens = 0\n",
    "    unique_tokens = 0\n",
    "    avg_token_len = 0.0\n",
    "    lex_diversity = 0.0\n",
    "    top_10 = Counter()\n",
    "    \n",
    "    # Do your tokenization and normalization here\n",
    "    \n",
    "    text = [w.lower() for w in text.split() if w.lower() not in sw and w.isalpha()]\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Calculate your statistics here\n",
    "    total_tokens = len(text)\n",
    "    unique_tokens = len(set(text))\n",
    "    avg_token_len = sum(len(w) for w in text) / len(text)\n",
    "    lex_diversity = unique_tokens/total_tokens\n",
    "    text_count= Counter(text)\n",
    "    top_10 = text_count.most_common(10)\n",
    "    \n",
    "    \n",
    "    # Now we'll fill out the dictionary. \n",
    "    \n",
    "    results = {'tokens':total_tokens,\n",
    "               'unique_tokens':unique_tokens,\n",
    "               'avg_token_length':avg_token_len,\n",
    "               'lexical_diversity':lex_diversity,\n",
    "               'top_10':top_10}\n",
    "\n",
    "    return(results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokens': 9288,\n",
       " 'unique_tokens': 2970,\n",
       " 'avg_token_length': 5.37521533161068,\n",
       " 'lexical_diversity': 0.31976744186046513,\n",
       " 'top_10': [('beowulf', 58),\n",
       "  ('men', 56),\n",
       "  ('shall', 48),\n",
       "  ('son', 47),\n",
       "  ('thou', 47),\n",
       "  ('king', 46),\n",
       "  ('lord', 45),\n",
       "  ('one', 45),\n",
       "  ('many', 44),\n",
       "  ('thy', 44)]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_patterns(beowulf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "beowulf = open(\"beowulf.txt\",'r').read()\n",
    "big_word_file = open(\"big.txt\",'r').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "beo_results = get_patterns(beowulf)\n",
    "big_results = get_patterns(big_word_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you've finished the assignment, these next cells should run without raising any assertion errors. It's always possible that _I_ have a mistake in my version, so if you think you've done everything correctly and you're still getting errors, let me know. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed all assertion tests.\n"
     ]
    }
   ],
   "source": [
    "assert(beo_results['tokens']==9288)\n",
    "assert(beo_results['unique_tokens']==2970)\n",
    "assert(0.31 < beo_results['lexical_diversity'] < 0.32)\n",
    "assert(5.3 < beo_results['avg_token_length'] < 5.4)\n",
    "assert(len(beo_results['top_10'])==10)\n",
    "assert(beo_results['top_10'][4]==('thou',47))\n",
    "print(\"Passed all assertion tests.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed all assertion tests.\n"
     ]
    }
   ],
   "source": [
    "assert(big_results['tokens']==410950)\n",
    "assert(big_results['unique_tokens']==25173)\n",
    "assert(0.06 < big_results['lexical_diversity'] < 0.065)\n",
    "assert(6.3 < big_results['avg_token_length'] < 6.4)\n",
    "assert(len(big_results['top_10'])==10)\n",
    "assert(big_results['top_10'][0]==('said',2946))\n",
    "assert(big_results['top_10'][8]==('upon',1088))\n",
    "print(\"Passed all assertion tests.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "All of the important bits of the assignment are above here. Everything below is designed to help you wrap your mind around what I'm asking for. \n",
    "\n",
    "\n",
    "### Dictionaries\n",
    "\n",
    "We'll talk about dictionary objects in class a bit, but think of them as a big bag of key-value pairs. The keys, which have to be unique, are like an address to the data that's being stored in the \"value\", which can be any kind of object. They're incredibly versatile and useful objects to learn about. Here's a [video](https://www.youtube.com/watch?v=daefaLgNkw0) that will help orient you to them. \n",
    "\n",
    "In the next couple of cells, I'll make a couple of dictionaries and show you some basic functionality with them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "animal_weights = {\"mouse\":0.03,\"dog\":20,\"horse\":700,\"elephant\":4100,\"blue whale\":100000}\n",
    "\n",
    "animal_weights['dog']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in animal_weights :\n",
    "    print(key)\n",
    "    print(animal_weights[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will throw an error\n",
    "animal_weights['cat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "animal_weights['cat'] = 3\n",
    "\n",
    "animal_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for animal, wt in animal_weights.items() :\n",
    "    print(f\"An average {animal} weighs {wt} kilograms ({wt/2.2:0.2f} lbs).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions\n",
    "\n",
    "Functions are a useful way to organize code and prevent yourself from making mistakes. There is a lot to learn about functions, but we'll just handle the basics here. Functions are declared with the `def` statement (for \"define\", I assume). The parentheses allow you to define the _parameters_ that are passed into the function. Here's a simple example that squares a number. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_square(x) :\n",
    "    return(x*x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_square(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_square(102)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_square(\"cat\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can add some error handling to make sure we only try to square a number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def square(x) :\n",
    "    if not str(x).isnumeric() :\n",
    "        raise ValueError(f\"Hey, {x} isn't a number!\")\n",
    "    else :\n",
    "        return(x*x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "square(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "square(\"cat\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's an example of a function that returns a dictionary based on an NLTK text being sent in. (Note that those texts are in lists.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(text) :\n",
    "    # Take in a text, return the first token, the 100th token, and the last token.\n",
    "    \n",
    "    first_token = text[0]\n",
    "    hundredth_token = text[99]\n",
    "    last_token = text[-1]\n",
    "    \n",
    "    ret_dict = {'first':first_token,\n",
    "                '100th':hundredth_token,\n",
    "                'last':last_token}\n",
    "    return(ret_dict)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(process_text(text1))\n",
    "print(process_text(text5))\n",
    "print(process_text(text8))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
